# -*- coding: utf-8 -*-
"""6416878_Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JV7QXKjlyXhKoKhucOpoFmbVFrPAAlNH
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set default Seaborn style for better visuals
sns.set_style("whitegrid")

df = pd.read_csv("BankChurners.csv")

print(df.count())

# Check the first few rows of the dataset
print(df.head())

 # Check data types and non-null counts
print(df.info())

# Get basic statistical summary
print(df.describe())

# Check for null or missing values
print(df.isnull().sum())

# Check for duplicate rows
print(df.duplicated().sum())

# Example: dropping unnecessary columns
df = df.drop(['CLIENTNUM'], axis=1)  # assuming 'CLIENTNUM' is not needed for analysis

# Plot the distribution of Attrition_Flag (target variable)
sns.countplot(data=df, x='Attrition_Flag')
plt.title('Distribution of Attrition_Flag')

# Add the count on top of each bar
attrition_counts = df['Attrition_Flag'].value_counts()
for index, value in enumerate(attrition_counts):
    plt.text(index, value, str(value), ha='center', va='bottom')

plt.show()

# Plot the distribution of Gender
sns.countplot(data=df, x='Gender')
plt.title('Distribution of Gender')

# Add the count on top of each bar
for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width() / 2,  # x-coordinate: center of the bar
             p.get_height() + 5,             # y-coordinate: slightly above the bar
             int(p.get_height()),             # value to display
             ha='center', va='bottom')       # horizontal and vertical alignment

plt.show()

# Define the desired order for Education_Level
education_order = [
    'High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate', 'Uneducated', 'Unknown'
]

# Set the figure size
plt.figure(figsize=(15, 10))  # Increase the plot size

# Plot the distribution of Education_Level in the specified order
sns.countplot(data=df, x='Education_Level', order=education_order)
plt.title('Distribution of Education Level')

# Add the count on top of each bar
for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width() / 2,    # x-coordinate: center of the bar
             p.get_height() + 50,             # y-coordinate: slightly above the bar, adjust if needed
             int(p.get_height()),              # value to display
             ha='center', va='bottom', fontsize=12)  # align and set font size

# Show the plot
plt.show()

#Bivariate Analysis
#Categorical vs Categorical
# Gender vs Attrition_Flag

# Set the figure size
plt.figure(figsize=(10, 6))  # Customize the size as needed

# Plot the countplot for Gender vs Attrition_Flag
sns.countplot(data=df, x='Gender', hue='Attrition_Flag')
plt.title('Gender vs Attrition Flag')

# Add the count on top of each bar
for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width() / 2,    # x-coordinate: center of the bar
             p.get_height() + 10,              # y-coordinate: slightly above the bar
             int(p.get_height()),              # count to display
             ha='center', va='bottom', fontsize=12)  # align and set font size

# Show the plot
plt.show()

# Define the desired order for Education_Level
education_order = [
    'High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate', 'Uneducated', 'Unknown'
]

# Set the figure size
plt.figure(figsize=(10, 6))  # Customize the size as needed

# Plot the countplot for Education_Level vs Attrition_Flag with the specified order
sns.countplot(data=df, x='Education_Level', hue='Attrition_Flag', order=education_order)
plt.title('Education Level vs Attrition Flag')

# Add the count on top of each bar
for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width() / 2,    # x-coordinate: center of the bar
             p.get_height() + 10,             # y-coordinate: slightly above the bar
             int(p.get_height()),             # count to display
             ha='center', va='bottom', fontsize=12)  # align and set font size

# Show the plot
plt.show()

# Define the desired order for Marital_Status
marital_status_order = ['Single', 'Married', 'Divorced', 'Unknown']

# Set the figure size
plt.figure(figsize=(10, 6))  # Customize the size as needed

# Plot the countplot for Marital_Status vs Attrition_Flag with the specified order
sns.countplot(data=df, x='Marital_Status', hue='Attrition_Flag', order=marital_status_order)
plt.title('Marital Status vs Attrition Flag')

# Add the count on top of each bar
for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width() / 2,    # x-coordinate: center of the bar
             p.get_height() + 10,             # y-coordinate: slightly above the bar
             int(p.get_height()),             # count to display
             ha='center', va='bottom', fontsize=12)  # align and set font size

# Show the plot
plt.show()

# Define the desired order for Income_Category
income_category_order = ['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +', 'Unknown']

# Set the figure size
plt.figure(figsize=(10, 6))  # Customize the size as needed

# Plot the countplot for Income_Category vs Attrition_Flag with the specified order
sns.countplot(data=df, x='Income_Category', hue='Attrition_Flag', order=income_category_order)
plt.title('Income Category vs Attrition Flag')

# Add the count on top of each bar
for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width() / 2,    # x-coordinate: center of the bar
             p.get_height() + 10,             # y-coordinate: slightly above the bar
             int(p.get_height()),             # count to display
             ha='center', va='bottom', fontsize=12)  # align and set font size

# Show the plot
plt.show()

# Create age bins
bins = [25, 32, 39, 46, 53, 60, 73]  # Upper limits for each bin
labels = ['26-32', '33-39', '40-46', '47-53', '54-60', '61-73']  # Labels for bins

# Create a new column for age groups
df['Age_Group'] = pd.cut(df['Customer_Age'], bins=bins, labels=labels)

# Set the figure size for the plot
plt.figure(figsize=(10, 6))

# Plot the countplot for Age_Group vs Attrition_Flag
sns.countplot(data=df, x='Age_Group', hue='Attrition_Flag')
plt.title('Age Group vs Attrition Flag')

# Add the count on top of each bar
for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width() / 2,
             p.get_height() + 10,
             int(p.get_height()),
             ha='center', va='bottom', fontsize=12)

# Show the plot
plt.show()

# Create age bins of size 10
bins = [25, 35, 45, 55, 65, 73]  # Adjusted for 10-year intervals
labels = ['26-35', '36-45', '46-55', '56-65', '66-73']  # Labels for the bins

# Create a new column for age groups
df['Age_Group'] = pd.cut(df['Customer_Age'], bins=bins, labels=labels)

# Set the figure size for the plot
plt.figure(figsize=(10, 6))

# Plot the countplot for Age_Group vs Attrition_Flag
sns.countplot(data=df, x='Age_Group', hue='Attrition_Flag')
plt.title('Age Group vs Attrition Flag')

# Add the count on top of each bar
for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width() / 2,
             p.get_height() + 10,
             int(p.get_height()),
             ha='center', va='bottom', fontsize=12)

# Show the plot
plt.show()

from sklearn.preprocessing import LabelEncoder

# Copy the dataframe to avoid modifying the original data
df_encoded = df.copy()

# List of categorical columns to encode (you can specify or use select_dtypes)
categorical_columns = ['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']

# Apply Label Encoding to the specified categorical columns
label_encoder = LabelEncoder()
for col in categorical_columns:
    df_encoded[col] = label_encoder.fit_transform(df_encoded[col])

# Check the new dataframe with encoded variables
print(df_encoded.head())

# Use the cleaned data from df_filtered
target_col = 'Attrition_Flag'

# Move the target column 'Attrition_Flag' to the end
df_filtered = df_filtered[[col for col in df_filtered.columns if col != target_col] + [target_col]]

# Display the first few rows to confirm
df_filtered.head()

from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Normalize 'Months_on_book' and update the column
df_filtered['Months_on_book'] = scaler.fit_transform(df_filtered[['Months_on_book']])

# Display the updated column
print(df_filtered['Months_on_book'].head())

# Define age bins and labels
bins = [25, 35, 45, 55, 65, 73]  # Upper limits for each bin
labels = [1, 2, 3, 4, 5]  # Assign numerical labels for each range

# Create a new column for age groups
df_filtered['Age_Range'] = pd.cut(df_filtered['Customer_Age'], bins=bins, labels=labels, right=True)

# Drop the original 'Customer_Age' column if it won't be used
df_filtered = df_filtered.drop('Customer_Age', axis=1)

columns = df_filtered.columns.tolist()  # Get the list of all column names
columns.insert(1, columns.pop(columns.index('Age_Range')))  # Remove 'Age_Range' and insert it at position 1
df_filtered = df_filtered[columns]  # Reorder the DataFrame based on the new column order

# Display the first few rows to confirm
df_filtered.head()

# Step 1: Make a copy of the original DataFrame
df_copy = df.copy()

# Step 2: Remove the last 7 columns
df_copy = df_copy.iloc[:, :-10]

# # Step 3: Move the target column 'Attrition_Flag' to the end
target_col = 'Attrition_Flag'
df_copy = df_copy[[col for col in df_copy.columns if col != target_col] + [target_col]]

# Step 4: Display the first few rows to confirm
df_copy.head()

"""## Model and Training"""

from sklearn.model_selection import train_test_split

# Separate the features and the target variable
X = df_filtered.iloc[:, :-1]  # Features (all columns except the target)
y = df_filtered['Attrition_Flag']  # Target (the last column)

# Split the data into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Display the shapes of the resulting splits
print(f"Training features shape: {X_train.shape}")
print(f"Testing features shape: {X_test.shape}")
print(f"Training target shape: {y_train.shape}")
print(f"Testing target shape: {y_test.shape}")

import matplotlib.pyplot as plt

# Data for the pie chart
labels = ['Training Data (70%)', 'Testing Data (30%)']
sizes = [len(X_train), len(X_test)]  # Sizes of the splits
colors = ['#ff9999', '#66b3ff']  # Optional colors for better visuals

# Create the pie chart
plt.figure(figsize=(8, 6))  # Set the figure size
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
plt.title('Dataset Split: Training vs Testing')
plt.axis('equal')  # Equal aspect ratio ensures that the pie chart is a circle
plt.show()

#Handling Imbalance with SMOTE

from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the training set
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Check the distribution of the target after SMOTE
print("Target distribution before SMOTE:")
print(y_train.value_counts())
print("\nTarget distribution after SMOTE:")
print(pd.Series(y_train_smote).value_counts())

# Create a DataFrame for visualization
before_smote = pd.DataFrame({'Class': y_train.value_counts().index, 'Count': y_train.value_counts().values})
after_smote = pd.DataFrame({'Class': pd.Series(y_train_smote).value_counts().index,
                            'Count': pd.Series(y_train_smote).value_counts().values})

# Set up the figure for subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)

# Plot before SMOTE
sns.barplot(data=before_smote, x='Class', y='Count', ax=axes[0], palette='muted')
axes[0].set_title('Class Distribution Before SMOTE')
axes[0].set_xlabel('Class')
axes[0].set_ylabel('Count')

# Plot after SMOTE
sns.barplot(data=after_smote, x='Class', y='Count', ax=axes[1], palette='muted')
axes[1].set_title('Class Distribution After SMOTE')
axes[1].set_xlabel('Class')

# Adjust the layout
plt.tight_layout()
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Logistic Regression Before SMOTE
logistic_model_before = LogisticRegression(random_state=42, max_iter=500)
logistic_model_before.fit(X_train, y_train)  # Train on imbalanced data
y_pred_before = logistic_model_before.predict(X_test)  # Predict on test set

# Confusion Matrix Before SMOTE
conf_matrix_before = confusion_matrix(y_test, y_pred_before)
accuracy_before = accuracy_score(y_test, y_pred_before)

# Logistic Regression After SMOTE
logistic_model_after = LogisticRegression(random_state=42, max_iter=500)
logistic_model_after.fit(X_train_smote, y_train_smote)  # Train on SMOTE-balanced data
y_pred_after = logistic_model_after.predict(X_test)  # Predict on test set

# Confusion Matrix After SMOTE
conf_matrix_after = confusion_matrix(y_test, y_pred_after)
accuracy_after = accuracy_score(y_test, y_pred_after)

# Plot Confusion Matrices
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Confusion Matrix Before SMOTE
sns.heatmap(conf_matrix_before, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axes[0])
axes[0].set_title(f'Before SMOTE\nAccuracy: {accuracy_before:.2f}')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

# Confusion Matrix After SMOTE
sns.heatmap(conf_matrix_after, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axes[1])
axes[1].set_title(f'After SMOTE\nAccuracy: {accuracy_after:.2f}')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

plt.tight_layout()
plt.show()

# Print Accuracy Scores
print(f"Accuracy Before SMOTE: {accuracy_before:.2f}")
print(f"Accuracy After SMOTE: {accuracy_after:.2f}")

#Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Initialize the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)

# Train the model on the balanced training data
dt_model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred_dt = dt_model.predict(X_test)

# Evaluate the model's performance
print("Decision Tree Model Performance:")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt))

print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_dt))

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

# Decision Tree Before SMOTE
dt_model_before = DecisionTreeClassifier(random_state=42)
dt_model_before.fit(X_train, y_train)  # Train on imbalanced data
y_pred_before_dt = dt_model_before.predict(X_test)  # Predict on test set

# Confusion Matrix Before SMOTE
conf_matrix_before_dt = confusion_matrix(y_test, y_pred_before_dt)
accuracy_before_dt = accuracy_score(y_test, y_pred_before_dt)

# Decision Tree After SMOTE
dt_model_after = DecisionTreeClassifier(random_state=42)
dt_model_after.fit(X_train_smote, y_train_smote)  # Train on SMOTE-balanced data
y_pred_after_dt = dt_model_after.predict(X_test)  # Predict on test set

# Confusion Matrix After SMOTE
conf_matrix_after_dt = confusion_matrix(y_test, y_pred_after_dt)
accuracy_after_dt = accuracy_score(y_test, y_pred_after_dt)

# Plot Confusion Matrices
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Confusion Matrix Before SMOTE
sns.heatmap(conf_matrix_before_dt, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axes[0])
axes[0].set_title(f'Before SMOTE\nAccuracy: {accuracy_before_dt:.2f}')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

# Confusion Matrix After SMOTE
sns.heatmap(conf_matrix_after_dt, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axes[1])
axes[1].set_title(f'After SMOTE\nAccuracy: {accuracy_after_dt:.2f}')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

plt.tight_layout()
plt.show()

# Print Accuracy Scores
print(f"Accuracy Before SMOTE: {accuracy_before_dt:.2f}")
print(f"Accuracy After SMOTE: {accuracy_after_dt:.2f}")

#Random Forest Classifer
from sklearn.ensemble import RandomForestClassifier

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the balanced training data
rf_model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model's performance
print("Random Forest Model Performance:")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_rf))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Random Forest Before SMOTE
rf_model_before = RandomForestClassifier(random_state=42)
rf_model_before.fit(X_train, y_train)  # Train on imbalanced data
y_pred_before_rf = rf_model_before.predict(X_test)  # Predict on test set

# Confusion Matrix Before SMOTE
conf_matrix_before_rf = confusion_matrix(y_test, y_pred_before_rf)
accuracy_before_rf = accuracy_score(y_test, y_pred_before_rf)

# Random Forest After SMOTE
rf_model_after = RandomForestClassifier(random_state=42)
rf_model_after.fit(X_train_smote, y_train_smote)  # Train on SMOTE-balanced data
y_pred_after_rf = rf_model_after.predict(X_test)  # Predict on test set

# Confusion Matrix After SMOTE
conf_matrix_after_rf = confusion_matrix(y_test, y_pred_after_rf)
accuracy_after_rf = accuracy_score(y_test, y_pred_after_rf)

# Plot Confusion Matrices
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Confusion Matrix Before SMOTE
sns.heatmap(conf_matrix_before_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axes[0])
axes[0].set_title(f'Before SMOTE\nAccuracy: {accuracy_before_rf:.2f}')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

# Confusion Matrix After SMOTE
sns.heatmap(conf_matrix_after_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axes[1])
axes[1].set_title(f'After SMOTE\nAccuracy: {accuracy_after_rf:.2f}')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

plt.tight_layout()
plt.show()

# Print Accuracy Scores
print(f"Accuracy Before SMOTE: {accuracy_before_rf:.2f}")
print(f"Accuracy After SMOTE: {accuracy_after_rf:.2f}")

#Gradient Boosting Classifier (XGBoost)
from xgboost import XGBClassifier

# Initialize the XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Train the model on the balanced training data
xgb_model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model's performance
print("XGBoost Model Performance:")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_xgb))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))

print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_xgb))

# Support Vector Machine (SVM)

from sklearn.svm import SVC

# Initialize the SVM model
svm_model = SVC(kernel='rbf', random_state=42)

# Train the model on the balanced training data
svm_model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred_svm = svm_model.predict(X_test)

# Evaluate the model's performance
print("SVM Model Performance:")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm))

print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_svm))

#K-Nearest Neighbors (KNN)
from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)

# Train the model on the balanced training data
knn_model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred_knn = knn_model.predict(X_test)

# Evaluate the model's performance
print("KNN Model Performance:")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_knn))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_knn))

print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_knn))

#Compare Model Performances

import pandas as pd

# Collect performance metrics
models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost', 'SVM', 'KNN']
accuracy = [
    accuracy_score(y_test, y_pred),
    accuracy_score(y_test, y_pred_dt),
    accuracy_score(y_test, y_pred_rf),
    #accuracy_score(y_test, y_pred_xgb),
    accuracy_score(y_test, y_pred_svm),
    accuracy_score(y_test, y_pred_knn)
]

# Assuming you have stored precision, recall, and F1-score for the minority class (class 0)
from sklearn.metrics import precision_score, recall_score, f1_score

precision = [
    precision_score(y_test, y_pred, pos_label=0),
    precision_score(y_test, y_pred_dt, pos_label=0),
    precision_score(y_test, y_pred_rf, pos_label=0),
    #precision_score(y_test, y_pred_xgb, pos_label=0),
    precision_score(y_test, y_pred_svm, pos_label=0),
    precision_score(y_test, y_pred_knn, pos_label=0)
]

recall = [
    recall_score(y_test, y_pred, pos_label=0),
    recall_score(y_test, y_pred_dt, pos_label=0),
    recall_score(y_test, y_pred_rf, pos_label=0),
    #recall_score(y_test, y_pred_xgb, pos_label=0),
    recall_score(y_test, y_pred_svm, pos_label=0),
    recall_score(y_test, y_pred_knn, pos_label=0)
]

f1 = [
    f1_score(y_test, y_pred, pos_label=0),
    f1_score(y_test, y_pred_dt, pos_label=0),
    f1_score(y_test, y_pred_rf, pos_label=0),
    #f1_score(y_test, y_pred_xgb, pos_label=0),
    f1_score(y_test, y_pred_svm, pos_label=0),
    f1_score(y_test, y_pred_knn, pos_label=0)
]

# Create a DataFrame to display the results
performance_df = pd.DataFrame({
    'Model': models,
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1
})

print(performance_df)

#Random Forest Feature Importance
import matplotlib.pyplot as plt

# Get feature importances from the Random Forest model
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
feature_names = X_train.columns

# Plot the feature importances
plt.figure(figsize=(12, 6))
plt.title("Feature Importances by Random Forest")
plt.bar(range(X_train.shape[1]), importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), feature_names[indices], rotation=90)
plt.tight_layout()
plt.show()

#Hyperparameter Tuning
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'max_features': ['auto', 'sqrt'],
    'min_samples_split': [2, 5, 10]
}

# Initialize the Grid Search with cross-validation
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='f1')

# Fit the grid search to the data
grid_search.fit(X_train_smote, y_train_smote)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Use the best estimator to make predictions
best_rf_model = grid_search.best_estimator_
y_pred_best_rf = best_rf_model.predict(X_test)

# Evaluate the optimized model
print("Optimized Random Forest Model Performance:")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_best_rf))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_best_rf))

print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_best_rf))

"""New Revised Code"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv("BankChurners.csv")

df.head()

# Step 1: Drop unnecessary columns
df.drop(columns=['CLIENTNUM', 'Unnamed: 21'], inplace=True, errors='ignore')

df.head()

# Step 2: Encode binary categorical variable 'Gender' using LabelEncoder
label_encoder = LabelEncoder()
df['Gender'] = label_encoder.fit_transform(df['Gender'])  # F=0, M=1

df.head()

# Step 3: One-Hot Encode other categorical features
df = pd.get_dummies(df, columns=['Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category'], drop_first=True)

df.head()

# Step 4: Display cleaned dataset structure
df_info_cleaned = df.info()
df_head_cleaned = df.head()

df_info_cleaned, df_head_cleaned

# Step 4: Encode target variable
df['Attrition_Flag'] = df['Attrition_Flag'].apply(lambda x: 1 if x == 'Attrited Customer' else 0)

from sklearn.preprocessing import MinMaxScaler

# Step 5: Normalize numeric columns (excluding the target)
numeric_cols = df.select_dtypes(include=['int64', 'float64']).drop(columns=['Attrition_Flag']).columns
scaler = MinMaxScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

df.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Compute correlation matrix
corr_matrix = df.corr()

# Plot the heatmap
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show()

#Define features and target
X = df.drop(columns=['Attrition_Flag'])
y = df['Attrition_Flag']

# Step 3: Train-Test Split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

#Apply Smote
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Step 5: Display shapes and class distribution
print("Original class distribution in y_train:")
print(y_train.value_counts())

print("\nAfter SMOTE class distribution:")
print(y_train_smote.value_counts())

print("\nTraining shape after SMOTE:", X_train_smote.shape)
print("Testing shape:", X_test.shape)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_smote)
X_test_scaled = scaler.transform(X_test)

# Logistic Regression after SMOTE with scaled features

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize the model
log_model = LogisticRegression(random_state=42, max_iter=1000)

# Train the model on the scaled, balanced data
log_model.fit(X_train_scaled, y_train_smote)

# Make predictions on the scaled test set
y_pred_log = log_model.predict(X_test_scaled)

# Evaluate performance
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_log))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_log))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred_log):.2f}")

# Plot confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_log), annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.title("Logistic Regression - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize the model
dt_model = DecisionTreeClassifier(random_state=42, max_depth=5)  # You can tweak max_depth

# Train the model
dt_model.fit(X_train_smote, y_train_smote)

# Predict on test set
y_pred_dt = dt_model.predict(X_test)

# Evaluate performance
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt))

print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_dt))

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title("Decision Tree - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# RANDOM FOREST CLASSIFIER

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_smote, y_train_smote)

# Predict on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluation
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
report_rf = classification_report(y_test, y_pred_rf)

# Display results
print("Confusion Matrix:")
print(conf_matrix_rf)

print("\nClassification Report:")
print(report_rf)

print("\nAccuracy Score:")
print(accuracy_rf)

#Plot Confusion Matrix
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Random Forest - Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# XGBoost Classifier
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize the XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Train the model on the balanced training data (after SMOTE)
xgb_model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model's performance
conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)

print("Confusion Matrix:")
print(conf_matrix_xgb)

print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))

print("\nAccuracy Score:")
print(accuracy_xgb)

# Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.title('XGBoost - Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize the MLP Classifier
mlp_model = MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers with 64 and 32 neurons
                          activation='relu',
                          solver='adam',
                          max_iter=300,
                          random_state=42)

# Train the model on the balanced training data
mlp_model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred_mlp = mlp_model.predict(X_test)

# Evaluate performance
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_mlp))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_mlp))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_mlp))

# Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_mlp)
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.title("MLP Neural Network - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize and train the model
lgbm_model = LGBMClassifier(random_state=42)
lgbm_model.fit(X_train_smote, y_train_smote)

# Predict on test data
y_pred_lgbm = lgbm_model.predict(X_test)

# Evaluate
conf_matrix_lgbm = confusion_matrix(y_test, y_pred_lgbm)
accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)

print("Confusion Matrix:")
print(conf_matrix_lgbm)
print("\nClassification Report:")
print(classification_report(y_test, y_pred_lgbm))
print("\nAccuracy Score:")
print(accuracy_lgbm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_lgbm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title(f"LightGBM - Confusion Matrix\nAccuracy Score: {accuracy_lgbm:.2f}")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Define model names
models = [
    'Logistic Regression',
    'Decision Tree',
    'Random Forest',
    'XGBoost',
    'MLP Neural Network',
    'LightGBM'
]

# Performance metrics for Class 1 (Attrited Customers)
accuracy =    [0.86, 0.90, 0.95, 0.97, 0.90, 0.96]
precision =   [0.54, 0.64, 0.84, 0.91, 0.69, 0.90]
recall =      [0.81, 0.84, 0.83, 0.88, 0.69, 0.87]
f1_score =    [0.65, 0.72, 0.83, 0.89, 0.69, 0.89]

# Create DataFrame
df_metrics = pd.DataFrame({
    'Model': models,
    'Accuracy': accuracy,
    'Precision (Class 1)': precision,
    'Recall (Class 1)': recall,
    'F1-Score (Class 1)': f1_score
})

# Plotting
plt.figure(figsize=(14, 8))
bar_width = 0.2
x = range(len(models))

plt.bar([p - 1.5*bar_width for p in x], df_metrics['Accuracy'], width=bar_width, label='Accuracy')
plt.bar([p - 0.5*bar_width for p in x], df_metrics['Precision (Class 1)'], width=bar_width, label='Precision')
plt.bar([p + 0.5*bar_width for p in x], df_metrics['Recall (Class 1)'], width=bar_width, label='Recall')
plt.bar([p + 1.5*bar_width for p in x], df_metrics['F1-Score (Class 1)'], width=bar_width, label='F1-Score')

plt.xticks(x, models, rotation=45, ha='right')
plt.ylabel('Score')
plt.title('Model Comparison on Class 1 (Attrited Customers)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Define model names
models = [
    'Logistic Regression',
    'Decision Tree',
    'Random Forest',
    'XGBoost',
    'MLP Neural Network',
    'LightGBM'
]

# Performance metrics for Class 1 (Attrited Customers)
accuracy =    [0.86, 0.90, 0.95, 0.97, 0.90, 0.96]
precision =   [0.54, 0.64, 0.84, 0.91, 0.69, 0.90]
recall =      [0.81, 0.84, 0.83, 0.88, 0.69, 0.87]
f1_score =    [0.65, 0.72, 0.83, 0.89, 0.69, 0.89]

# Create DataFrame
df_metrics = pd.DataFrame({
    'Model': models,
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1_score
})

# Plotting
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
x = np.arange(len(metrics))  # [0, 1, 2, 3]
bar_width = 0.13

plt.figure(figsize=(12, 8))

for i, model in enumerate(df_metrics['Model']):
    plt.bar(x + i*bar_width,
            df_metrics.loc[i, metrics],
            width=bar_width,
            label=model)

plt.xticks(x + bar_width * (len(models)-1)/2, metrics)
plt.ylabel('Score')
plt.title('Performance Comparison Across Models')
plt.ylim(0, 1.05)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

# Extract actual feature names
feature_names = X_train.columns.tolist()

# Get raw importance scores
rf_importances = rf_model.feature_importances_
xgb_importances = xgb_model.feature_importances_
lgbm_importances = lgbm_model.feature_importances_

# Combine into DataFrame
importances_df = pd.DataFrame({
    'Feature': feature_names,
    'Random Forest': rf_importances,
    'XGBoost': xgb_importances,
    'LightGBM': lgbm_importances
})

# Normalize all importance columns (scale 0â€“1)
scaler = MinMaxScaler()
importances_df[['Random Forest', 'XGBoost', 'LightGBM']] = scaler.fit_transform(
    importances_df[['Random Forest', 'XGBoost', 'LightGBM']]
)

# Sort by LightGBM (or any column you prefer)
importances_df = importances_df.sort_values(by='LightGBM', ascending=True).set_index('Feature')

# Plot
plt.figure(figsize=(14, 12))
importances_df.plot(kind='barh', figsize=(14, 12), colormap='Set2')
plt.title('Normalized Feature Importances by Model')
plt.xlabel('Normalized Importance Score')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# 1. Get actual feature names
feature_names = X_train.columns.tolist()

# 2. Extract importances from models
rf_importances = rf_model.feature_importances_
xgb_importances = xgb_model.feature_importances_
lgbm_importances = lgbm_model.feature_importances_

# 3. Combine into DataFrame
importances_df = pd.DataFrame({
    'Feature': feature_names,
    'Random Forest': rf_importances,
    'XGBoost': xgb_importances,
    'LightGBM': lgbm_importances
}).set_index('Feature')

# 4. Normalize for visibility
scaler = MinMaxScaler()
importances_df[:] = scaler.fit_transform(importances_df)

# 5. Get top 5 features by mean importance
importances_df['Mean_Importance'] = importances_df.mean(axis=1)
top5_df = importances_df.sort_values(by='Mean_Importance', ascending=False).head(5).drop(columns='Mean_Importance')

# ----- CHART -----
plt.figure(figsize=(10, 6))
top5_df.plot(kind='barh', figsize=(10, 6), colormap='Set2')
plt.title('Top 5 Most Important Features (Normalized)')
plt.xlabel('Normalized Importance Score (0 to 1)')
plt.tight_layout()
plt.show()

# ----- PERCENTAGE TABLE -----
# Calculate relative contribution of each top feature
contribution_percent = top5_df.mean(axis=1) / importances_df.drop(columns='Mean_Importance').mean(axis=1).sum() * 100
contribution_df = contribution_percent.reset_index().rename(columns={0: 'Contribution (%)'})
contribution_df.sort_values(by='Contribution (%)', ascending=False, inplace=True)

# Display the table
print("Top 5 Feature Contributions:")
print(contribution_df.to_string(index=False))

import pandas as pd
import matplotlib.pyplot as plt

# âœ… Step 1: Use the exact column names from your dataset
selected_features = ['Customer_Age', 'Credit_Limit', 'Dependent_count', 'Gender']  # Update if needed

# âœ… Step 2: Filter the importances DataFrame
selected_df = importances_df.loc[selected_features, ['Random Forest', 'XGBoost', 'LightGBM']]

# âœ… Step 3: Plot bar chart
plt.figure(figsize=(10, 6))
selected_df.plot(kind='barh', figsize=(10, 6), colormap='Accent')
plt.title('Importance of Selected Features (Normalized)')
plt.xlabel('Normalized Importance Score (0 to 1)')
plt.tight_layout()
plt.show()

# âœ… Step 4: Show percentage contribution of these features
selected_contrib_percent = selected_df.mean(axis=1) / importances_df.drop(columns='Mean_Importance').mean(axis=1).sum() * 100
selected_contrib_df = selected_contrib_percent.reset_index().rename(columns={0: 'Contribution (%)'})
selected_contrib_df.sort_values(by='Contribution (%)', ascending=False, inplace=True)

# Display table
print("Selected Feature Contributions (%):")
print(selected_contrib_df.to_string(index=False))

!pip install lime

import lime
import lime.lime_tabular
import numpy as np

# STEP 1: Use your trained model â€” for example, XGBoost or LightGBM
model = xgb_model  # or rf_model, lgbm_model, etc.

# STEP 2: Reconstruct the original feature names used for training
feature_names = X_train.columns.tolist()

# STEP 3: Create a LIME explainer object
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=feature_names,
    class_names=['Existing Customer', 'Attrited Customer'],
    mode='classification',
    discretize_continuous=True
)

# STEP 4: Pick a row to explain (e.g., first row in the test set)
idx = 0
sample = X_test.iloc[idx].values.reshape(1, -1)

# STEP 5: Explain the prediction
explanation = explainer.explain_instance(
    data_row=X_test.iloc[idx],
    predict_fn=model.predict_proba,
    num_features=10  # number of top features to display
)

# STEP 6: Visualize explanation
explanation.show_in_notebook(show_table=True, show_all=False)

# Try explaining another customer
idx = 60  # ðŸ‘ˆ Change this to any row index from your test set

# Re-run LIME explanation
explanation = explainer.explain_instance(
    data_row=X_test.iloc[idx],
    predict_fn=model.predict_proba,
    num_features=10
)

# Show explanation
explanation.show_in_notebook(show_table=True, show_all=False)

# Step 1: Get predictions for the test set
y_pred = model.predict(X_test)

# Step 2: Find indexes where model predicts "1" = Attrited Customer
attrited_indexes = np.where(y_pred == 1)[0]

# Step 3: Display first few such customers
print(f"Total customers predicted to leave: {len(attrited_indexes)}")
print(f"Indexes of first few predicted attrited customers: {attrited_indexes[:10]}")

model = lgbm_model  # switch to your LightGBM model
# Predict on the test set using LightGBM
y_pred_lgbm = model.predict(X_test)

# Get the indexes of customers predicted to churn (class = 1)
attrited_indexes_lgbm = np.where(y_pred_lgbm == 1)[0]

print(f"Total customers LightGBM predicts to churn: {len(attrited_indexes_lgbm)}")
print(f"First few indexes: {attrited_indexes_lgbm[:10]}")

# Choose an index from the churned predictions
idx = attrited_indexes_lgbm[0]  # or any other index from the list

# Generate explanation using LIME
explanation = explainer.explain_instance(
    data_row=X_test.iloc[idx],
    predict_fn=model.predict_proba,  # LightGBM works with predict_proba too
    num_features=10
)

# Show the explanation
explanation.show_in_notebook(show_table=True, show_all=False)

idx = 52  # or any index of interest
sample = X_test.iloc[idx]
# LIME with XGBoost
explanation_xgb = explainer.explain_instance(
    data_row=sample,
    predict_fn=xgb_model.predict_proba,
    num_features=10
)
explanation_xgb.show_in_notebook(show_table=True, show_all=False)

# LIME with LightGBM
explanation_lgbm = explainer.explain_instance(
    data_row=sample,
    predict_fn=lgbm_model.predict_proba,
    num_features=10
)
explanation_lgbm.show_in_notebook(show_table=True, show_all=False)

import joblib

# Save trained model and scaler
joblib.dump(xgb_model, "xgboost_model.pkl")
joblib.dump(scaler, "scaler.pkl")

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 1],
    'reg_alpha': [0, 0.5],
    'reg_lambda': [1, 2]
}

# Initialize model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Grid search
grid_search = GridSearchCV(estimator=xgb,
                           param_grid=param_grid,
                           scoring='f1',
                           cv=3,
                           verbose=1,
                           n_jobs=-1)

grid_search.fit(X_train_smote, y_train_smote)

# Show best results
print("Best Parameters:", grid_search.best_params_)
print("Best F1 Score:", grid_search.best_score_)

# Re-evaluate on test set
best_xgb = grid_search.best_estimator_
y_pred_best_xgb = best_xgb.predict(X_test)

print("\nFinal Classification Report:")
print(classification_report(y_test, y_pred_best_xgb))